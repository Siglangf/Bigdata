{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project 1-1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zdDeFc9ybfZl",
        "GaLhicB3_XfK",
        "k0UyL8cIBqxk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siglangf/Bigdata/blob/main/Project_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdDeFc9ybfZl"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDfsY0JtJsg"
      },
      "source": [
        "Kodelinje for å installere Spark på i Google Collab. Trengs bare å kjøre én gang. Forhåpentligvis ligger den inne etter det"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psLoboeJo7zM"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPCSTicZRScG"
      },
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJWWXrnSq60D"
      },
      "source": [
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH7OF_QU8Wej"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTrEJzs8pCPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f25e09e-4408-4897-868f-3a6aa886b631"
      },
      "source": [
        "#For å legge til driven i notebooken så man kan få tilgang til filene som ligger i mappene\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXPMJ6zPG3eZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6bb954-bafe-4345-dfd1-7fd6e2fcbb23"
      },
      "source": [
        "!curl -L -o \"/content/spark-3.0.2-bin-hadoop2.7/jars/graphframes-0.8.0-spark3.0-s_2.12.jar\" http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.8.0-spark3.0-s_2.12/graphframes-0.8.0-spark3.0-s_2.12.jar\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  237k  100  237k    0     0   786k      0 --:--:-- --:--:-- --:--:-- 7900k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuG8WQDm0Fq_"
      },
      "source": [
        "\n",
        "\n",
        "Kodesnutten under må kjøres for å fortelle Python hvor den skal finne Spark filene"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "IwGYE75tpNC4",
        "outputId": "6a00dd27-4de8-44ac-f4fa-7dfa7f35e05b"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.2-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOR0PdP3pVek"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config(\"spark.jars.packages\", \"graphframes:graphframes:0.7.0-spark2.4-s_2.11\")\\\n",
        "        .getOrCreate()\n",
        "#Legg til denne for å få GraphFrame pakken\n",
        "spark.sparkContext.addPyFile(f'{os.environ[\"SPARK_HOME\"]}/jars/graphframes-0.8.0-spark3.0-s_2.12.jar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnF-4llSCJdW"
      },
      "source": [
        "#Legges inn som argument\n",
        "DATA_PATH = \"/content/drive/My Drive/Bigdata\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaLhicB3_XfK"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ4pDjmgABw0"
      },
      "source": [
        "Task 1.1-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLghcXY3sXTN"
      },
      "source": [
        "#Laster inn som en data\n",
        "posts = spark.read.csv(path=f\"{DATA_PATH}/posts.csv\",sep=r'\\t', header=True)\n",
        "comments = spark.read.csv(path=f\"{DATA_PATH}/comments.csv\",sep=r'\\t', header=True)\n",
        "users = spark.read.csv(path=f\"{DATA_PATH}/users.csv\",sep=r'\\t', header=True)\n",
        "badges = spark.read.csv(path=f\"{DATA_PATH}/badges.csv\",sep=r'\\t', header=True)\n",
        "\n",
        "posts = posts.rdd\n",
        "comments = comments.rdd\n",
        "users = users.rdd\n",
        "badges = badges.rdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8nTMjJNSDp4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA59rPBZ_0rK"
      },
      "source": [
        "Task 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLJUfo9_xxY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50X3MJX4xbiO",
        "outputId": "8a148116-5894-463a-b4e3-938d94116c3a"
      },
      "source": [
        "post_count = posts.count()\n",
        "comments_count = comments.count()\n",
        "users_count = users.count()\n",
        "badges_count = badges.count()\n",
        "print(post_count,comments_count,users_count,badges_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56217 58735 91616 105640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0UyL8cIBqxk"
      },
      "source": [
        "# Task 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTz7VzhLPMpx"
      },
      "source": [
        "Task 2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55BBXeBOPFHx"
      },
      "source": [
        "from pyspark.sql.functions import unbase64,base64\n",
        "import base64\n",
        "\n",
        "# Funksjon for å dekode base64-encoding\n",
        "def decode_line(base64_message):\n",
        "  base64_bytes = base64_message.encode('utf-8')\n",
        "  message_bytes = base64.b64decode(base64_bytes)\n",
        "  message = message_bytes.decode('utf-8')\n",
        "  return message\n",
        "\n",
        "# Dekoder kommentarer, finner lengden av dem, og lager nye rows med lengdene og ettall. \n",
        "# Deretter reduserer vi til en tupel med total lengde og antall kommentarer\n",
        "comments_keyvalue = comments.map(lambda line: (len(decode_line(line['Text'])), 1)) \\\n",
        "              .reduce(lambda a, b : (a[0]+b[0], a[1]+b[1])) \\\n",
        "\n",
        "comments_avg = comments_length_count[0]/comments_length_count[1]\n",
        "\n",
        "print(\"The average length of the comments is: \" + str(comments_avg))\n",
        "\n",
        "#Key/Value-pair bestående av PostTypeId og Body\n",
        "post_id_body = posts.map(lambda line: (int(line['PostTypeId']), decode_line(line['Body'])))\n",
        "\n",
        "# Gjør først om verdiene til tupler bestående av lengden på strengene og et ettall. Deretter summerer vi opp lengdene og countene \n",
        "# og finner snitt ved å dele lengde på count. \n",
        "avg_by_key = post_id_body.mapValues(lambda v: (len(v), 1)) \\\n",
        "            .reduceByKey(lambda a,b : ((a[0])+(b[0]), a[1]+b[1])) \\\n",
        "            .mapValues(lambda v: v[0]/v[1]) \\\n",
        "            .collectAsMap() \n",
        "\n",
        "print(\"The average length of question-posts is: \" + str(avg_by_key[1]))\n",
        "print(\"The average length of answer-posts is: \" + str(avg_by_key[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiT7VrkWPXW6"
      },
      "source": [
        "Task 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rci1q2igPWjb"
      },
      "source": [
        "#Find the dates when the first and the last questions were asked. Also, find the display name of users who posted those questions\n",
        "question_dates = posts.filter(lambda line : line['PostTypeId']==\"1\") \\\n",
        "                      .map(lambda line : (line['CreationDate'], line['OwnerUserId'])) \\\n",
        "                      .sortBy(lambda line : line[0], ascending = True)\n",
        "first_date = question_dates.first()\n",
        "#print(\"The first date is: \" + str(first_date[0]))\n",
        "\n",
        "question_dates = question_dates.sortBy(lambda line : line[0], ascending = False)\n",
        "last_date = question_dates.first()\n",
        "#print(\"The last date is: \" + str(last_date[0]))\n",
        "\n",
        "user_first_date = users.filter(lambda line : line['Id'] == first_date[1]) \\\n",
        "                       .map(lambda line: line['DisplayName'])\n",
        "\n",
        "user_last_date = users.filter(lambda line : line['Id'] == last_date[1]) \\\n",
        "                       .map(lambda line: line['DisplayName'])\n",
        "\n",
        "print(\"The user \" + user_first_date.take(1)[0] + \" asked the first question at \" + str(first_date[0]))\n",
        "print(\"The user \" + user_last_date.take(1)[0] + \" asked the last question at \" + str(last_date[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ5mHt4O1bzW"
      },
      "source": [
        "Task 2.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyV5azsDboW_",
        "outputId": "ad427fd8-5575-4a3b-e0a7-f886a025cfd2"
      },
      "source": [
        "u = posts.map(lambda line: (line['OwnerUserId'],1))\n",
        "u = u.filter(lambda line: line[0] != '-1')\n",
        "u = u.reduceByKey(lambda x,y: int(x) + int(y))\n",
        "counts = u.map(lambda line: line[1])\n",
        "max_posts = counts.reduce(lambda x,y: max(x,y))\n",
        "u = u.filter(lambda line: line[1]==max_posts)\n",
        "u.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('64377', 579)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFxMHvmK1Y7P"
      },
      "source": [
        "Task 2.4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUj4PbuwCZqz",
        "outputId": "432a5807-d0d4-4e35-8ea7-9631267fddc6"
      },
      "source": [
        "users_and_badges = badges.map(lambda badge: (badge['UserId'], 1))\n",
        "num_badges = users_and_badges.reduceByKey(lambda x,y: int(x)+int(y))\n",
        "above_3 = num_badges.filter(lambda x: x[1]>=3)\n",
        "number_of_users = above_3.count()\n",
        "print(f\"{number_of_users} has above 3 badges\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9763 has above 3 badges\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2eR3cU0QMOB"
      },
      "source": [
        "Task 2.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_57nN_w2Cnkt"
      },
      "source": [
        "from operator import add\n",
        "upvotes_user_map = users.map(lambda user: (user['Id'], user['UpVotes']))\n",
        "#Drow None values and -1 user\n",
        "upvotes_user_map = upvotes_user_map.filter(lambda user: not user[1]==None and not user[0]=='-1')\n",
        "\n",
        "\n",
        "downvotes_user_map = users.map(lambda user: (user['Id'], user['DownVotes']))\n",
        "#Drop none values and -1 user\n",
        "downvotes_user_map = downvotes_user_map.filter(lambda user: not user[1]==None and not user[0]==-1)\n",
        "\n",
        "\n",
        "#Compute mean of upvotes per user\n",
        "upvotes = upvotes_user_map.map(lambda user: (int(user[1])))\n",
        "agg_upvotes = upvotes.reduce(add)\n",
        "mean_upvotes = agg_upvotes/upvotes_user_map.count()\n",
        "\n",
        "\n",
        "#Compute mean of downvotes per user\n",
        "downvotes = downvotes_user_map.map(lambda user: int(user[1]))\n",
        "agg_downvotes = downvotes.reduce(add)\n",
        "mean_downvotes = agg_downvotes/upvotes_user_map.count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "AnJyQ4V0bj5w",
        "outputId": "2b7bcac6-ef7e-4288-c99d-e7cbf9d20cbd"
      },
      "source": [
        "from math import sqrt,pow\n",
        "user_map = users.map(lambda user: (user['Id'],(user['UpVotes'],user['DownVotes'])))\n",
        "user_map = user_map.filter(lambda user: not user[0]=='-1' and not user[1][0]==None and not user[1][1]==None)\n",
        "variance = user_map.map(lambda user: (user[0], (int(user[1][0])-mean_upvotes)*(int(user[1][1])-mean_downvotes)))\n",
        "variance_sum = variance.map(lambda user: user[1]).reduce(add)\n",
        "\n",
        "upvote_squared = user_map.map(lambda user: (int(user[1][0])-mean_upvotes)**2)\n",
        "downvote_squared = user_map.map(lambda user: (int(user[1][1])-mean_downvotes)**2)\n",
        "\n",
        "upvote_squared_sum = upvote_squared.reduce(add)\n",
        "downvote_squared_sum = downvote_squared.reduce(add)\n",
        "\n",
        "pearson = variance_sum/(sqrt(upvote_squared_sum)*sqrt(downvote_squared_sum))\n",
        "\n",
        "pearson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a76ff3219882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muser_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UpVotes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DownVotes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muser_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'-1'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_upvotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_downvotes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvariance_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'users' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE7qnjS6r9IC"
      },
      "source": [
        "Task 2.6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w96NW8lr7Ww",
        "outputId": "c12fd3ae-9a76-452b-ab3a-dc7e4abebe39"
      },
      "source": [
        "from math import log2\n",
        "comment_map = comments.map(lambda comment: (comment['UserId'], 1))\n",
        "comments_per_user = comment_map.reduceByKey(add)\n",
        "total_comments = comment_map.count()\n",
        "probability_per_user = comments_per_user.map(lambda user: (user[0],user[1]/total_comments))\n",
        "entropy_user = probability_per_user.map(lambda prob: prob[1]*log2(prob[1]))\n",
        "entropy = -entropy_user.reduce(add)\n",
        "entropy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.257354251781877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX4RN7WAbKWC"
      },
      "source": [
        "# Task 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MRynW71Qj2L"
      },
      "source": [
        "posts = spark.read.csv(path=f\"{DATA_PATH}/posts.csv\",sep=r'\\t', header=True)\n",
        "comments = spark.read.csv(path=f\"{DATA_PATH}/comments.csv\",sep=r'\\t', header=True)\n",
        "users = spark.read.csv(path=f\"{DATA_PATH}/users.csv\",sep=r'\\t', header=True)\n",
        "\n",
        "posts = posts.rdd\n",
        "comments = comments.rdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDWldutqRnbF"
      },
      "source": [
        "Task 3.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3mcZc8YQ1yP"
      },
      "source": [
        "#Make key/value pairs with postid and userIDs\n",
        "posts_map = posts.filter(lambda line: line[\"OwnerUserId\"]!=\"NULL\").map(lambda post: (int(post['Id']), int(post['OwnerUserId'])))\n",
        "comments_map = comments.filter(lambda line: line[\"UserId\"]!=\"NULL\").map(lambda comment: (int(comment['PostId']), int(comment['UserId'])))\n",
        "#Joining on postid to get \"PosterId and CommentID pairs\"\n",
        "joined =posts_map.join(comments_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpIFvbJ-Rc7I"
      },
      "source": [
        "#Counting number of equal PosterId and CommentID pairs\n",
        "counts = joined.map(lambda line: (line[1],1))\n",
        "counts = counts.reduceByKey(lambda x,y: x+y)\n",
        "#Creating graph tuples formated as (Posterid,Commenterid, Weight)\n",
        "graph = counts.map(lambda line: (line[0][0],line[0][1],line[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1PWMK_eT0pm"
      },
      "source": [
        "Task 3.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R2bFG1Z1Sq0"
      },
      "source": [
        "df = graph.toDF([\"PosterId\",\"CommenterId\",\"Weight\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbE18CEm7dW9",
        "outputId": "3c21717f-e918-4686-c872-9c7377e8bb30"
      },
      "source": [
        "import pyspark.sql.functions as sf\n",
        "top_ten_commenters = df.groupBy(\"CommenterId\").agg(sf.sum(\"Weight\").alias(\"Num_comments\")).orderBy(\"Num_comments\",ascending=False).limit(10)\n",
        "top_ten_commenters = top_ten_commenters.select(top_ten_commenters.CommenterId.alias(\"Id\"),\"Num_comments\")\n",
        "top_ten_commenters.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|   Id|Num_comments|\n",
            "+-----+------------+\n",
            "|  836|        1263|\n",
            "|  381|        1226|\n",
            "|28175|         864|\n",
            "|64377|         746|\n",
            "|35644|         693|\n",
            "|55122|         631|\n",
            "|  924|         505|\n",
            "|71442|         493|\n",
            "|   21|         406|\n",
            "|45264|         402|\n",
            "+-----+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGWZnRy5_xLF"
      },
      "source": [
        "Task 3.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2g14nv1-Q7B"
      },
      "source": [
        "#Finding userdi with most comments on posts\n",
        "top_ten_posters = df.groupBy(\"PosterId\").agg(sf.sum(\"Weight\").alias(\"Num_comments\")).orderBy(\"Num_comments\",ascending=False).limit(10)\n",
        "top_ten_posters = top_ten_posters.select(top_ten_posters.PosterId.alias(\"Id\"),\"Num_comments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrgPH0xaUByJ"
      },
      "source": [
        "Task 3.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkZr7047lCK"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "#Converting Userids from string to Int\n",
        "users = users.withColumn(\"Id\", users[\"Id\"].cast(IntegerType())).select(\"Id\",\"DisplayName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzbJ5OBU9Bx5",
        "outputId": "4f211d95-6792-4864-fe30-ccba68611556"
      },
      "source": [
        "#Joining with user table to find DisplayName\n",
        "names = top_ten_posters.join(users, on=\"Id\").select(\"DisplayName\",\"Num_comments\").orderBy(\"Num_comments\",ascending=False)\n",
        "names.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+------------+\n",
            "|         DisplayName|Num_comments|\n",
            "+--------------------+------------+\n",
            "|         Neil Slater|         875|\n",
            "|               Erwan|         729|\n",
            "|               Media|         703|\n",
            "|             n1k31t4|         622|\n",
            "|Has QUIT--Anony-M...|         502|\n",
            "|            JahKnows|         362|\n",
            "|               Leevo|         349|\n",
            "|         David Masip|         318|\n",
            "|          Noah Weber|         315|\n",
            "|      Brian Spiering|         313|\n",
            "+--------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE8S7n59UL-x"
      },
      "source": [
        "Task 3.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbiiVHoW-D05"
      },
      "source": [
        "csv = graph.map(lambda line: \",\".join([str(l) for l in line]))\n",
        "csv.saveAsTextFile(f\"{DATA_PATH}/graph.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}